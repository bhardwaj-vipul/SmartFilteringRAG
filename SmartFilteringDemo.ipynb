{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483d04c8-3d20-442c-9263-b349d52f8de7",
   "metadata": {},
   "source": [
    "## Initialize MongoDB vector database with some sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e4be0b-9fb3-4a1b-aac5-025cdc22bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the notebook after installing the packages\n",
    "\n",
    "!pip install pymongo==4.7.2 langchain-core==0.2.6 langchain-openai==0.1.7 langchain==0.2.1 langchain-community==0.2.4 lark==1.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e32b7e0-47a4-4a74-8885-0494682f9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = os.getenv('MONGO_URI')\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "DB_NAME = \"your database name\"\n",
    "COLLECTION_NAME = \"your collection name\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5677993-8c33-4e72-9cd7-a563f59a4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"release_date\": \"1994-04-15\", \"rating\": 7.7, \"genre\": [\"action\", \"scifi\", \"adventure\"]},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"release_date\": \"2010-07-16\", \"director\": \"Christopher Nolan\", \"rating\": 8.2, \"genre\": [\"action\", \"thriller\"]},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"release_date\": \"2006-11-25\", \"director\": \"Satoshi Kon\", \"rating\": 8.6, \"genre\": [\"anime\", \"thriller\", \"scifi\"]},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"release_date\": \"2019-12-25\", \"director\": \"Greta Gerwig\", \"rating\": 8.3, \"genre\": [\"romance\", \"drama\", \"comedy\"]},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"release_date\": \"1995-11-22\", \"genre\": [\"anime\", \"fantasy\"]},\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28fe15d6-d5c7-434c-8b5e-84d4ff5339c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPEN_API_BASE\")\n",
    "\n",
    "# default_headers is optional\n",
    "default_headers = os.getenv(\"OPEN_API_DEFAULT_HEADERS\")\n",
    "default_headers = json.loads(default_headers) if default_headers else None\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key, openai_api_base=openai_api_base, default_headers=default_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13718f1-9010-453b-b371-f72ccd0705ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(docs, embeddings, collection=collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d1004-34bd-4533-ad30-9577683edeae",
   "metadata": {},
   "source": [
    "## Create a vector search index with name `default` and use the below in the JSON editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709c6dc6-df22-4de0-b1af-e689db0e348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'numDimensions': 1536,\n",
       "   'path': 'embedding',\n",
       "   'similarity': 'cosine',\n",
       "   'type': 'vector'},\n",
       "  {'path': 'genre', 'type': 'filter'},\n",
       "  {'path': 'rating', 'type': 'filter'},\n",
       "  {'path': 'release_date', 'type': 'filter'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy below while creating the vector search index\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"numDimensions\": 1536,\n",
    "      \"path\": \"embedding\",\n",
    "      \"similarity\": \"cosine\",\n",
    "      \"type\": \"vector\"\n",
    "    },\n",
    "    {\n",
    "      \"path\": \"genre\",\n",
    "      \"type\": \"filter\"\n",
    "    },\n",
    "    {\n",
    "      \"path\": \"rating\",\n",
    "      \"type\": \"filter\"\n",
    "    },\n",
    "    {\n",
    "      \"path\": \"release_date\",\n",
    "      \"type\": \"filter\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100601d9-348b-478e-8502-c353a6b3a3d7",
   "metadata": {},
   "source": [
    "## Define the metadata for the MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c363dfb-0273-4c0f-8efe-b1a5af8b6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"Keywords for filtering: ['anime', 'action', 'comedy', 'romance', 'thriller']\",\n",
    "        type=\"[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"release_date\",\n",
    "        description=\"The date the movie was released on\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d9b90-9a94-4f21-bd61-09c3394f99b6",
   "metadata": {},
   "source": [
    "## Define a tool that we will use for running queries on our MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa44f960-b35d-4249-9209-44cb0410abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import traceback\n",
    "from typing import Dict, Optional, Type, Union, List\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "class MongoDBClient:\n",
    "    \"\"\"Data helper for querying MongoDB Vector Indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "\n",
    "    def run_aggregate_pipeline(self, pipeline: List[Dict]) -> List[Dict]:\n",
    "        documents = list(self.collection.aggregate(pipeline))\n",
    "        return documents\n",
    "\n",
    "class BaseMongoDBTool(BaseModel):\n",
    "    \"\"\"Base tool for interacting with MongoDB.\"\"\"\n",
    "\n",
    "    client: MongoDBClient = Field(exclude=True)\n",
    "    match_filter: dict = Field(exclude=True)\n",
    "\n",
    "    class Config(BaseTool.Config):\n",
    "        pass\n",
    "\n",
    "class _QueryExecutorMongoDBToolInput(BaseModel):\n",
    "    pipeline: str = Field(..., description=\"A valid MongoDB pipeline in JSON string format\")\n",
    "\n",
    "class QueryExecutorMongoDBTool(BaseMongoDBTool, BaseTool):\n",
    "    name: str = \"mongo_db_executor\"\n",
    "    description: str = \"\"\"\n",
    "    Input to this tool is a mongodb pipeline, output is a list of documents.\n",
    "    If the pipeline is not correct, an error message will be returned.\n",
    "    If an error is returned, report back to the user the issue and stop.\n",
    "    \"\"\"\n",
    "    args_schema: Type[BaseModel] = _QueryExecutorMongoDBToolInput\n",
    "\n",
    "    def _run(\n",
    "            self,\n",
    "            pipeline: str,\n",
    "            run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> Union[List[Dict], str]:\n",
    "        \"\"\"Get the result for the mongodb pipeline.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Pipeline: {pipeline}/\")\n",
    "            logger.info(f\"Match filter: {self.match_filter}/\")\n",
    "            pipeline = json.loads(pipeline)\n",
    "            if self.match_filter:\n",
    "                pipeline = [{\"$match\": self.match_filter}] + pipeline\n",
    "            logger.info(f\"Updated pipeline: {pipeline}/\")\n",
    "            documents = self.client.run_aggregate_pipeline(pipeline)\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            \"\"\"Format the error message\"\"\"\n",
    "            return f\"Error: {e}\\n{traceback.format_exc()}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43491b7f-8f0b-49bd-9002-a15b9f579ec0",
   "metadata": {},
   "source": [
    "## Define prompt and examples that we will be using for the Query Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a6f19-c0cb-405d-b24c-9ab7bdd54c3d",
   "metadata": {},
   "source": [
    "> Note: Update the prompt and examples as per your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2f9a0b-1279-45c1-96a1-cb986385065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_SCHEMA = \"\"\"\\\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{{{\n",
    "    \"query\": string \\\\ rewritten user's query after removing the information handled by the filter\n",
    "    \"filter\": string \\\\ logical condition statement for filtering documents\n",
    "}}}}\n",
    "```\n",
    "\n",
    "The query string should be re-written. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "A logical condition statement is composed of one or more comparison and logical operation statements.\n",
    "\n",
    "A comparison statement takes the form: `comp(attr, val)`:\n",
    "- `comp` ({allowed_comparators}): comparator\n",
    "- `attr` (string):  name of attribute to apply the comparison to\n",
    "- `val` (string): is the comparison value\n",
    "\n",
    "A logical operation statement takes the form `op(statement1, statement2, ...)`:\n",
    "- `op` ({allowed_operators}): logical operator\n",
    "- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n",
    "\n",
    "Make sure that you only use the comparators and logical operators listed above and no others.\n",
    "Make sure that filters only refer to attributes that exist in the data source.\n",
    "Make sure that filters only use the attributed names with its function names if there are functions applied on them.\n",
    "Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\n",
    "Make sure you understand the user's intent while generating a date filter. Use a range comparators such as gt | gte | lt | lte  for partial dates. \n",
    "Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored. \n",
    "Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\\\n",
    "\"\"\"\n",
    "DEFAULT_SCHEMA_PROMPT = PromptTemplate.from_template(DEFAULT_SCHEMA)\n",
    "\n",
    "SONG_DATA_SOURCE = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"content\": \"Lyrics of a song\",\n",
    "    \"attributes\": {{\n",
    "        \"artist\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Name of the song artist\"\n",
    "        }},\n",
    "        \"length\": {{\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Length of the song in seconds\"\n",
    "        }},\n",
    "        \"genre\": {{\n",
    "            \"type\": \"[string]\",\n",
    "            \"description\": \"The song genre, one or many of [\\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"]\"\n",
    "        }},\n",
    "        \"release_dt\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Release date of the song.\"\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "MEMO_DATA_SOURCE = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"content\": \"Estaff memos\",\n",
    "    \"attributes\": {{\n",
    "        \"memo_date\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Date the memo was published on\"\n",
    "        }},\n",
    "        \"title\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The title of the memo\"\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "KEYWORDS_DATA_SOURCE = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"content\": \"Documents store\",\n",
    "    \"attributes\": {{\n",
    "        \"tags\": {{\n",
    "            \"type\": \"[string]\",\n",
    "            \"description\": \"Keywords for filtering: ['credal', 'genai', 'radiant', 'langchain']\"\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "````\\\n",
    "\"\"\"\n",
    "\n",
    "KEYWORDS_DATA_SOURCE_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"Updates on radiant integration with credal\",\n",
    "    \"filter\": \"in(\\\\\"tags\\\\\", [\\\\\"credal\\\\\", \\\\\"radiant\\\\\"])\"\n",
    "}}\n",
    "````\\\n",
    "\"\"\"\n",
    "\n",
    "KEYWORDS_DATE_DATA_SOURCE_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"Updates based on most recent memo\",\n",
    "    \"filter\": \"in(\\\\\"tags\\\\\", [\\\\\"credal\\\\\", \\\\\"radiant\\\\\"])\"\n",
    "}}\n",
    "````\\\n",
    "\"\"\"\n",
    "\n",
    "FULL_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"songs about teenage romance\",\n",
    "    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), in(\\\\\"genre\\\\\", [\\\\\"pop\\\\\"]), and(gt(\\\\\"release_dt\\\\\", \\\\\"2010-12-31\\\\\"), lt(\\\\\"release_dt\\\\\", \\\\\"2020-01-01\\\\\")))\"\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "DATE_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"What are the updates on genai?\",\n",
    "    \"filter\": \"gt(\\\\\"memo_date\\\\\", \\\\\"2023-01-01\\\\\")\"\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "NO_FILTER_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"\",\n",
    "    \"filter\": \"NO_FILTER\"\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "WITH_LIMIT_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"love\",\n",
    "    \"filter\": \"NO_FILTER\",\n",
    "    \"limit\": 2\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_EXAMPLES = [\n",
    "    {\n",
    "        \"i\": 1,\n",
    "        \"data_source\": MEMO_DATA_SOURCE,\n",
    "        \"user_query\": \"What are the updates on genai after 1 Jan 2023\",\n",
    "        \"structured_request\": DATE_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 2,\n",
    "        \"data_source\": MEMO_DATA_SOURCE,\n",
    "        \"user_query\": \"What are the updates on genai\",\n",
    "        \"structured_request\": NO_FILTER_ANSWER\n",
    "    },\n",
    "    {\n",
    "        \"i\": 3,\n",
    "        \"data_source\": SONG_DATA_SOURCE,\n",
    "        \"user_query\": \"What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre released before 1 January 2020 and after 31 December, 2010\",\n",
    "        \"structured_request\": FULL_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 4,\n",
    "        \"data_source\": SONG_DATA_SOURCE,\n",
    "        \"user_query\": \"What are songs that were not published on Spotify\",\n",
    "        \"structured_request\": NO_FILTER_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 5,\n",
    "        \"data_source\": KEYWORDS_DATA_SOURCE,\n",
    "        \"user_query\": \"Updates on radiant integration in Credal\",\n",
    "        \"structured_request\": KEYWORDS_DATA_SOURCE_ANSWER\n",
    "    },\n",
    "    {\n",
    "        \"i\": 6,\n",
    "        \"data_source\": KEYWORDS_DATA_SOURCE,\n",
    "        \"user_query\": \"Updates on radiant integration in Credal based on most recent memo\",\n",
    "        \"structured_request\": KEYWORDS_DATE_DATA_SOURCE_ANSWER\n",
    "    }\n",
    "]\n",
    "\n",
    "EXAMPLES_WITH_LIMIT = [\n",
    "    {\n",
    "        \"i\": 1,\n",
    "        \"data_source\": SONG_DATA_SOURCE,\n",
    "        \"user_query\": \"What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre released before 1 January 2020 and after 31 December, 2010\",\n",
    "        \"structured_request\": FULL_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 2,\n",
    "        \"data_source\": SONG_DATA_SOURCE,\n",
    "        \"user_query\": \"What are songs that were not published on Spotify\",\n",
    "        \"structured_request\": NO_FILTER_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 3,\n",
    "        \"data_source\": SONG_DATA_SOURCE,\n",
    "        \"user_query\": \"What are three songs about love\",\n",
    "        \"structured_request\": WITH_LIMIT_ANSWER,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def enforce_constraints(input_json):\n",
    "    def process_value(value):\n",
    "        if isinstance(value, (str, int)):\n",
    "            return value\n",
    "        elif isinstance(value, list) and all(isinstance(item, str) for item in value):\n",
    "            return value\n",
    "        elif isinstance(value, dict) and 'date' in value and isinstance(value['date'], str):\n",
    "            return value['date']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value type\")\n",
    "\n",
    "    def process_dict(d):\n",
    "        if not isinstance(d, dict):\n",
    "            return d\n",
    "        processed_dict = {}\n",
    "        for k, v in d.items():\n",
    "            if k.startswith(\"$\") and isinstance(v, list):\n",
    "                # Handling $and and $or conditions\n",
    "                processed_dict[k] = [process_dict(item) for item in v]\n",
    "            elif k.startswith(\"$\"):\n",
    "                processed_dict[k] = process_value(v)\n",
    "            else:\n",
    "                processed_dict[k] = process_dict(v)\n",
    "        return processed_dict\n",
    "\n",
    "    return process_dict(input_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0ecd-4e20-4106-9e3d-a1970ec8e715",
   "metadata": {},
   "source": [
    "## Define the prompt that we will be using for the Time Based query constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8a26a-4d5d-49da-b26b-5bf101efff27",
   "metadata": {},
   "source": [
    "> Note: Update the prompt as per your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc85a69e-59d8-4a44-adf8-e337f650e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "Your goal is to structure the user's query to match the request schema provided below.\n",
    "\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{{{\n",
    "    \"query\": string \\\\ rewritten user's query after removing the information handled by the filter\n",
    "    \"filter\": string \\\\ logical condition statement for filtering documents\n",
    "}}}}\n",
    "```\n",
    "\n",
    "The query string should be re-written. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "A logical condition statement is composed of one or more comparison and logical operation statements.\n",
    "\n",
    "A comparison statement takes the form: `comp(attr, val)`:\n",
    "- `comp` ('eq | ne | gt | gte | lt | lte | in | nin'): comparator\n",
    "- `attr` (string):  name of attribute to apply the comparison to\n",
    "- `val` (string): is the comparison value\n",
    "\n",
    "A logical operation statement takes the form `op(statement1, statement2, ...)`:\n",
    "- `op` ('and | or'): logical operator\n",
    "- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n",
    "\n",
    "First step is to think about whether the user question mentions anything about date or time related that require a lookup in the MongoDB database. Words like \"latest\", \"recent\", \"earliest\", \"first\", \"last\" etc. in the query means a look up could be required.\n",
    "If no lookup is required, return \"NO_FILTER\" for the filter value.\n",
    "\n",
    "If required, create a syntactically correct MongoDB aggregation pipeline using '$sort' and '$limit' operator to run.\n",
    "Use projection to only fetch the relevant date columns.\n",
    "Then look at the results of the aggregation pipeline and generate a date range query that can be used to filter relevant documents from the collection.\n",
    "\n",
    "Make sure to only generate date-based filters.\n",
    "Make sure to only generate the query if a user asks about a time based question such as latest, most recent and not mention a specific date time.\n",
    "Make sure that you only use the comparators and logical operators listed above and no others.\n",
    "Make sure that filters only refer to date/time attributes that exist in the data source.\n",
    "Make sure that filters only use the attributed names with its function names if there are functions applied on them.\n",
    "Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\n",
    "Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\n",
    "Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n",
    "Make sure the column names in the filter query are in double quotes.\n",
    "\n",
    "<< Data Source >>\n",
    "```json\n",
    "{{{{\n",
    "    \"content\": {content_description},\n",
    "    \"attributes\": {attribute_info}\n",
    "}}}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11707cf7-6938-41ad-bb4e-428f2bcb5f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05e472c1-a3f2-480e-af71-b241cbeee436",
   "metadata": {},
   "source": [
    "## Define the MetadataFilter class that we will use to generate pre_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7001f535-3b47-4101-baea-c13de5c0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.chains.query_constructor.base import AttributeInfo, _format_attribute_info, StructuredQueryOutputParser\n",
    "from langchain.chains.query_constructor.base import load_query_constructor_runnable\n",
    "from langchain_community.query_constructors.mongodb_atlas import MongoDBAtlasTranslator\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, HumanMessagePromptTemplate, \\\n",
    "    SystemMessagePromptTemplate\n",
    "from pymongo import MongoClient\n",
    "from dw_gai.common.enums import ChatModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MetadataFilter:\n",
    "    \"\"\"\n",
    "    MetadataFilter is responsible for generating a MongoDB pre-filter query based on the user query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, llm, metadata_field_info, document_content_description):\n",
    "        \"\"\"\n",
    "        Initialize the MetadataFilter with a pymongo collection\n",
    "        :param collection: Pymongo collection\n",
    "        :param llm\n",
    "        :param metadata_info: Dict of attribute_info and content_description\n",
    "        \"\"\"\n",
    "        self.collection = collection\n",
    "        self.llm = llm\n",
    "        self.dataset_query_constructor = {}\n",
    "        self.translator = MongoDBAtlasTranslator()\n",
    "        self.metadata_field_info = metadata_field_info\n",
    "        self.document_content_description = document_content_description\n",
    "    \n",
    "    def create_query_constructor(self):\n",
    "        \"\"\"\n",
    "        This method will create query constructor for the collection.\n",
    "        The query constructor is a chain with a prompt created using collection's metadata and content description.\n",
    "        This query constructor will be used to generate pre-filter for a user's query.\n",
    "        \"\"\"\n",
    "        query_constructor_run_name = \"query_constructor\"\n",
    "\n",
    "        chain_kwargs = {}\n",
    "        translator = MongoDBAtlasTranslator()\n",
    "        chain_kwargs[\"allowed_operators\"] = translator.allowed_operators\n",
    "        chain_kwargs[\"allowed_comparators\"] = translator.allowed_comparators\n",
    "        enable_limit = False\n",
    "\n",
    "        query_constructor = load_query_constructor_runnable(\n",
    "            llm=self.llm,\n",
    "            document_contents=document_content_description,\n",
    "            attribute_info=metadata_field_info,\n",
    "            enable_limit=enable_limit,\n",
    "            schema_prompt=DEFAULT_SCHEMA_PROMPT,\n",
    "            examples=EXAMPLES_WITH_LIMIT if enable_limit else DEFAULT_EXAMPLES,\n",
    "            **chain_kwargs,\n",
    "        )\n",
    "\n",
    "        query_constructor = query_constructor.with_config(\n",
    "            run_name=query_constructor_run_name\n",
    "        )\n",
    "\n",
    "        return query_constructor\n",
    "\n",
    "    def generate_metadata_filter(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        This method will use the query constructor and generate the pre-filters for a list of datasets.\n",
    "        :param query: User's query\n",
    "        :return (dict): Returns pre-filter and new query for each dataset.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"Answer the below question:\\n\n",
    "                Question: {query}\n",
    "                \"\"\"\n",
    "        query_constructor = self.create_query_constructor()\n",
    "\n",
    "        structured_query = {}\n",
    "        try:\n",
    "            structured_query = query_constructor.invoke(query)\n",
    "            logger.info(f\"Structured query: {structured_query}\")\n",
    "            new_query, new_kwargs = self.translator.visit_structured_query(structured_query)\n",
    "            pre_filter = enforce_constraints(new_kwargs)\n",
    "            logger.info(f\"Generated pre-filter query: {pre_filter}\")\n",
    "            logger.info(f\"Generated new query: {query} -> {new_query}\")\n",
    "            if pre_filter:\n",
    "                time_based_pre_filter, new_query = self.generate_time_based_filter(pre_filter, new_query)\n",
    "                if time_based_pre_filter:\n",
    "                    logger.info(f\"Merging metadata filter: {pre_filter}, and\\n\\t{time_based_pre_filter}\")\n",
    "                    pre_filter[\"pre_filter\"] = {\n",
    "                        \"$and\": [pre_filter[\"pre_filter\"], time_based_pre_filter[\"pre_filter\"]]}\n",
    "            logger.info(f\"Final pre-filter query: {pre_filter}\")\n",
    "            pre_filter = pre_filter[\"pre_filter\"] if pre_filter else {}\n",
    "            new_query = new_query if new_query else query\n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Failed while creating pre-filter: {ex}\")\n",
    "        return pre_filter, new_query\n",
    "\n",
    "    def generate_time_based_filter(self, pre_filter: Dict, query: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        This method is responsible for generating filter query for \"most recent\", \"latest\", \"earliest\" type of user\n",
    "        questions.\n",
    "        :param pre_filter: (Dict) metadata pre-filter query\n",
    "        :param query: (str) user query\n",
    "        :param dataset: (str) MongoDB collection name\n",
    "        :return: (Tuple[str, Dict]) Rewritten user question and time-based filter query\n",
    "        \"\"\"\n",
    "        client = MongoDBClient(collection=self.collection)\n",
    "        executor_tool = QueryExecutorMongoDBTool(client=client, match_filter=pre_filter[\"pre_filter\"])\n",
    "        tools = [executor_tool]\n",
    "        attribute_str = _format_attribute_info(self.metadata_field_info)\n",
    "        system_prompt_template = SYSTEM_PROMPT_TEMPLATE.format(attribute_info=attribute_str,\n",
    "                                                               content_description=self.document_content_description)\n",
    "\n",
    "        prompt = ChatPromptTemplate(input_variables=[\"agent_scratchpad\", \"input\"],\n",
    "                                    messages=[SystemMessagePromptTemplate(\n",
    "                                        prompt=PromptTemplate(input_variables=[], template=system_prompt_template)),\n",
    "                                        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "                                        HumanMessagePromptTemplate(\n",
    "                                            prompt=PromptTemplate(input_variables=[\"input\"],\n",
    "                                                                  template=\"{input}\")),\n",
    "                                        MessagesPlaceholder(variable_name=\"agent_scratchpad\")])\n",
    "\n",
    "\n",
    "        agent = create_tool_calling_agent(self.llm, tools, prompt)\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "        structured_query = agent_executor.invoke({\"input\": query})\n",
    "        allowed_attributes = []\n",
    "        for ainfo in metadata_field_info:\n",
    "            allowed_attributes.append(\n",
    "                ainfo.name if isinstance(ainfo, AttributeInfo) else ainfo[\"name\"]\n",
    "            )\n",
    "\n",
    "        output_parser = StructuredQueryOutputParser.from_components(\n",
    "            allowed_comparators=self.translator.allowed_comparators,\n",
    "            allowed_operators=self.translator.allowed_operators,\n",
    "            allowed_attributes=allowed_attributes\n",
    "        )\n",
    "        structured_query = output_parser.parse(structured_query[\"output\"])\n",
    "        logger.info(f\"Structured query: {structured_query}\")\n",
    "        new_query, new_kwargs = self.translator.visit_structured_query(structured_query)\n",
    "        time_based_pre_filter = enforce_constraints(new_kwargs)\n",
    "        logger.info(f\"Generated time based pre-filter query: {time_based_pre_filter}\")\n",
    "        logger.info(f\"Generated new query after time based filtering: {query} -> {new_query}\")\n",
    "        return time_based_pre_filter, new_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51475f31-77a6-4594-bc60-abd24b122c23",
   "metadata": {},
   "source": [
    "## Use the MetadataFilter and pass the 'pre_filter' in our MongoDBAtlasVectorSearch retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf628ec5-5443-4aa6-8b7b-29adf622d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, default_headers=default_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621abba2-240e-4db2-88c2-b5867514651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Use the following pieces of context to answer the user question in subsequent messages. The context was retrieved from a knowledge database and you should use only the facts from the context to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer, use the context. Don't address the context directly, but use it to answer the user question like it's your own knowledge.\n",
    "Context: ```{context}```\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt),\n",
    "                    (\"human\", \"{query}\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "query = \"I want to watch a movie released before year 2000 in the anime genre with the latest release date\"\n",
    "\n",
    "metadata_filter = MetadataFilter(collection=collection,\n",
    "                                 llm=llm, \n",
    "                                 metadata_field_info=metadata_field_info,\n",
    "                                 document_content_description=document_content_description)\n",
    "\n",
    "pre_filter, new_query = metadata_filter.generate_metadata_filter(query)\n",
    "query = new_query\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch( collection, embeddings )\n",
    "retriever = vectorStore.as_retriever(\n",
    "    search_kwargs={'pre_filter': pre_filter}\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30556775-90ec-46f8-b8dd-0fd12f682002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7c585-5e09-483a-baea-313f5cbcc266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
